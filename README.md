# Splunk

<h1>Log Analysis with Splunk</h1>
<body>Completed this analysis with help of Try Hack Me.</body></br>
<body>Room info : Splunk Basics - Did you SIEM?</body></br>
Room Type:
Free Room.Anyone can deploy virtual machines in the room


<h2>Description</h2>
Project consists of
Ingest and interpret custom log data in Splunk
Create and apply custom field extractions
Use Search Processing Language (SPL) to filter and refine search results
Conduct an investigation within Splunk to uncover key insights like,

- Exploring the Logs
- Initial Triage
- Visualizing the Logs Timeline
- Anomaly Detection
- Filtering out Benign Values
- Narrowing Down Suspicious IPs
- Tracing the Attack Chain
- Exfiltration Attempts
- Ransomware Staging & RCE
- Correlate Outbound C2 Communication
- Volume of Data Exfiltrated

<h2>Languages and Utilities Used</h2>

- <b>Search Processing Language (SPL)</b> 
- <b>Virtual Machines deployed in Try Hack Me</b>

<h2>Environments Used </h2>

- <b>Windows 11</b> 

<h2>Program walk-through:</h2>

<p align="left">
<b>Exploring the Logs:</b> <br/>
The data has already been ingested into the Splunk instance for analysis. To begin investigating the incident, open the Splunk interface and click Search & Reporting from the left-hand panel, as shown below.
<br />
<br />
<img width="1364" height="792" alt="Image" src="https://github.com/user-attachments/assets/b8c7ea1e-222d-414a-86ad-0c968bf436a1" />
<br />
<br />
On the next page, enter index=main in the search bar to display all the ingested logs.
<br />
<br />
<img width="1919" height="1199" alt="2" src="https://github.com/user-attachments/assets/1926c562-29bf-455c-a549-f0edda7ab08d" />
The investigation involves two primary datasets:<br />
  
- <b>web_traffic:</b> Contains events related to incoming and outgoing web connections associated with the web server.<br />
  
- <b>firewall_logs:</b> Includes firewall records that indicate whether network traffic was allowed or blocked.<br />
The web server is assigned the local IP address 10.10.1.15.
---------------------------------------------------------------------------------------------------------------------------------
<br />
<b>Initial Triage:</b> <br />
Begin by running a basic search on the index using the custom source type web_traffic. In the search bar, enter the following query:

<b>Search Query:</b> index=main sourcetype=web_traffic
<br />
<br />
<img width="1919" height="1199" alt="3" src="https://github.com/user-attachments/assets/0d22a7ae-c84a-48a7-954f-79b4ac2237f7" />
<br />
<br />
Let’s break down the results to better understand what we’re seeing in Splunk:

<b>Search query:</b> This query pulls all events from the main index that are tagged with the custom source type web_traffic. It serves as the starting point for our investigation and ensures we’re working with the correct dataset.

<b>Time range:</b> The search is currently set to All time, allowing us to view the complete dataset. During an actual security investigation, this range would typically be narrowed down later such as focusing on the specific time window where abnormal activity or traffic spikes were observed.

<b>Timeline:</b> The histogram visualizes how the 17,172 events are distributed over time. It shows normal daily log activity followed by a noticeable spike in traffic, which likely corresponds to the potential attack window.

<b>Selected fields:</b> These are the fields currently displayed in the event list summary (such as host, source, and sourcetype). They provide basic metadata about where the logs originated and how they were ingested.

<b>Interesting fields:</b> This panel displays all fields that Splunk has either automatically extracted or manually defined. Fields prefixed with # (for example, #date_hour) are automatically generated by Splunk’s time-based commands. The presence of fields like user_agent, path, and client_ip confirms that the web logs have been parsed correctly.

<b>Event details & field extraction:</b> This section shows the detailed breakdown of an individual event, including extracted fields such as user_agent, path, status, client_ip, and others. These fields will be critical for identifying suspicious behavior during the investigation.

Now that we have a solid understanding of the Splunk interface and how to interpret the logs, we can move forward with a deeper analysis of the dataset.

---------------------------------------------------------------------------------------------------------------------------------
<br />
<b>Visualizing the Logs Timeline:</b> <br />
Next, we’ll visualize the total number of events over time, grouped by day. This will allow us to see the daily event volume and quickly identify any day that experienced an unusually high number of logs, which may indicate abnormal or malicious activity.
<br />
<br />
<b>Search query:</b> index=main sourcetype=web_traffic | timechart span=1d count
<br />
<br />
<img width="1919" height="1199" alt="4" src="https://github.com/user-attachments/assets/9d5d21f0-36a8-425e-8cf5-c7ae74f2a6f8" />
<br />
The results above now display the number of events captured on a daily basis. This is particularly interesting, as we can observe that certain days have a noticeably higher volume of logs than others. To get a clearer and more intuitive view of this pattern, we can switch to the Visualization tab and examine the graph, as shown below.
<br />
<br />
<img width="1919" height="1199" alt="5" src="https://github.com/user-attachments/assets/e1b74fc4-fb62-4b14-9bb7-9c3b76209ad5" />
<br />
<br />
We can append the reverse function to the end of the query to sort the results in descending order. This places the day with the highest number of events at the top, making it easier to quickly identify periods of unusually high activity.

<b>Search query:</b>index=main sourcetype=web_traffic | timechart span=1d count | sort by count | reverse 
<br />
<br />
<img width="1919" height="1199" alt="7" src="https://github.com/user-attachments/assets/b718db0d-322c-4726-9ee0-9f04124971d0" />

---------------------------------------------------------------------------------------------------------------------------------
<br />
<b>Anomaly Detection:</b><br /><br />
Now that we’ve identified the days with unusually high log activity using both the table and the visualization, we can continue our investigation by examining specific fields for suspicious values. To do this, we’ll return to the Events tab and use the same search query to dig deeper into the data.
<b>User Agent</b>
Let's click on the user_agent field in the left panel, as shown below. It will show us the details about the user agents captured so far. 
<br/>
<br/>
<img width="1919" height="1199" alt="8" src="https://github.com/user-attachments/assets/9e94116e-faf0-4763-b2f4-7085012b3104" />
<br/>
<br/>
Upon closer inspection, we can see that alongside legitimate user agents such as standard Mozilla variants there is a significant number of suspicious user agents present in the logs. These anomalous entries warrant further investigation to determine whether they are associated with malicious or automated activity.
<br />
<br />
<b>client_ip</b><br/><br/>
The second field we will examine is the client_ip, which contains the IP addresses of the clients accessing the web server. We can immediately see one particular IP address standing out, which we will investigate further.
<br/>
<br/>
<img width="1919" height="1199" alt="9" src="https://github.com/user-attachments/assets/6cd72648-798d-40fa-977b-2b1db9ccab02" />
<br/>
<br/>
<b>path</b>
<br/>
<br/>
The third field we will examine is path, which contains the URI being requested and accessed by the client IPs. The results shown below clearly indicate some attacks worth investigating.
<br/><br/>
<img width="1919" height="1199" alt="10" src="https://github.com/user-attachments/assets/c44f727f-630e-430f-9ea2-98691c15baec" />
<br/>
<br/>
<b>Filtering out Benign Values:</b>









<!--
 ```diff
- text in red
+ text in green
! text in orange
# text in gray
@@ text in purple (and bold)@@
```
--!>
